{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Install Libraries","metadata":{}},{"cell_type":"code","source":"pip install -q accelerate peft bitsandbytes transformers trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\ntorch.cuda.empty_cache()\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer # Supervised Fine Tuning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reformatting instruction dataset according to llama template","metadata":{}},{"cell_type":"code","source":"# model_name = \"NousResearch/Llama-2-7b-chat-hf\"\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\ndataset_name = \"mlabonne/guanaco-llama2-1k\"\nnew_model = \"Llama-2-7b-chat-finetune\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#QLoRA Parameters\n#Lora attention Dimension\nlora_r = 64 #(rank)\n\n#Alpha parameter for Lora scaling\nlora_alpha = 16\n\n#Dropout Probability for Lora Layers\nlora_dropout = 0.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bitsandbytes parameters\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TrainingArguments Parameters\n\n#output Dir for model predictions and checkpoints\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with A100)\nfp16 = False\nbf16 = False\n\n# Batch Size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch Size per GPU for evalution\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for \ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# optimizer\noptimizer = \"paged_adamw_32bit\"\n\n# Learning rate scheduler \nlr_scheduler_type = \"cosine\"\n\n# Number of training steps (overrides num_train_epcohs)\nmax_steps = -1\n\n#Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length - saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 0\n\n# Log every X updates steps\nlogging_steps = 25","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SFT Parameters\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# ** Pack multiple short examples in same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on GPU 0 \ndevice_map = {\"\": 0}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load datasets and models and start fine tuning","metadata":{}},{"cell_type":"code","source":"# load dataset \ndataset = load_dataset(dataset_name, split=\"train\")\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit = use_4bit,\n    bnb_4bit_quant_type = bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype = compute_dtype,\n    bnb_4bit_use_double_quant = use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"GPU Supports bfloat16: accelerate training with bf16 = True\")\n        print(\"=\" * 80)\n        \n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config = bnb_config,\n    device_map = device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load LoRA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code= True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n#Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha = lora_alpha,\n    lora_dropout = lora_dropout,\n    r = lora_r,\n    bias = \"none\",\n    task_type = \"CAUSAL_LM\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir = output_dir,\n    num_train_epochs = num_train_epochs,\n    per_device_train_batch_size = per_device_train_batch_size,\n    gradient_accumulation_steps = gradient_accumulation_steps,\n    optim = optimizer,\n    save_steps = save_steps,\n    logging_steps = logging_steps,\n    learning_rate = learning_rate,\n    weight_decay = weight_decay,\n    fp16 = fp16,\n    bf16 = bf16,\n    max_grad_norm = max_grad_norm,\n    max_steps = max_steps,\n    warmup_ratio = warmup_ratio,\n    group_by_length = group_by_length,\n    lr_scheduler_type = lr_scheduler_type,\n    report_to=\"tensorboard\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set SFT (Supervised Fine Tuning) Trainer Parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset = dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length = max_seq_length,\n    tokenizer= tokenizer,\n    args=training_arguments,\n    packing=packing,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}